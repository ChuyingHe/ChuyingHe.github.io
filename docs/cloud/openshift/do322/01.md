
Course Objectives: Installing OpenShift on a cloud, virtual, or physical infrastructure.


# 1. Installation Methods


!!! note "Operations"
    - Day-0 - Planning & Alignment: Architecture design, Tool selection, Infrastructure Planning, Security & Compliance
    - Day-1 - Deployment(Automation): Infrastructure provisioning, Code deployment, Database Setup, Security configurations
    - Day-2 - Maintenance, Monitoring, and Optimization: Monitoring and Alerting, Incident response, Routine maintenance, Performance tuning

To deploy an OpenShift Container Platform cluster, administrators must use the openshift-install binary. 

To deploy an OpenShift Container Platform cluster, administrators must use the `openshift-install` binary(a.k.a. **The OpenShift Installer**). This cmd then generates **ignition configuration files**（字面意思：点火文件） for the bootstrap, control plane nodes, and compute nodes.

!!! info "where to find Ignition File?"
    The ignition process loads the ignition file configuration files from one of the following locations:

    - Local disk
    - Cloud metadata
    - Over the network using HTTP or HTTPS

!!! info "Example of Ignition File"
    For more information check [here](https://coreos.github.io/ignition/examples/).

    ```json
    {
    "ignition": {
        "version": "3.1.0"  # Ignition version
    },
    "passwd": {             # Configures SSH public keys for the user core.
        "users": [
        {
            "name": "core",
            "sshAuthorizedKeys": [
            "ssh-rsa AAA...hlw== lab@utility.lab.example.com\n",
            "ssh-rsa AAA...3DR\n"
            ]
        }
        ]
    },
    "storage": {            # Storage section
        "files": [
        {
    ...output omitted...
        {
            "overwrite": false,
            "path": "/etc/motd",  # Appends to the /etc/motd file the base64 encoded source data.
            "user": {
            "name": "root"
            },
            "append": [
            {
                "source": "data:text/plain;charset=utf-8;base64,VGh...lCg=="
            }
            ],
            "mode": 420
        },
    ...output omitted...

    # Creates a systemd service unit called `bootkube.service` that executes the script:
    #   `/usr/local/bin/bootkube.sh`
    "systemd": {
        "units": [
        {
            "contents": "[Unit]\nDescription=Bootstrap a Kubernetes cluster\nRequires=crio-configure.service\nWants=kubelet.service\nAfter=kubelet.service crio-configure.service\nConditionPathExists=!/opt/openshift/.bootkube.done\n\n[Service]\nWorkingDirectory=/opt/openshift\nExecStart=/usr/local/bin/bootkube.sh\n\nRestart=on-failure\nRestartSec=5s\n",
            "name": "bootkube.service"
        },
    ...output omitted...
        }
        ]
    }
    }
    ```
By default, the data stored in ignition files is encoded in **base64**. For inspection, you can decode this data in plain text using the `base64 -d` command:

```bash
[user@demo ~]$ echo "VGh...lCg==" | base64 -d
This is the **bootstrap node**; it will be destroyed when the master is fully up.
The primary services are release-image.service followed by bootkube.service. To watch their status, run e.g.

  journalctl -b -f -u release-image.service -u bootkube.service
```

Check the ignition configs used in the system:
```bash
cat /boot/ignition/config.ign
```

Chekc the ignition logs:
```bash
journalctl -t ignition
```

## Methods
The **OpenShift Installer** offers the following installation methods:

### 1. IPI / Full-stack Automation 
- also called installer-provisioned infrastructure (**IPI**).
- administrators install OpenShift with <ins>minimal manual intervention</ins> in an opinionated manner.
- The OpenShift installer deploys the cluster on infrastructure that the installer provisions and the cluster maintains.
- The OpenShift installer controls all installation areas, including <ins>infrastructure provisioning</ins>

<img src="../imgs/Full-stack-automation.svg" />

### 2. UPI / Pre-existing Infrastructure
- also called user-provisioned infrastructure (**UPI**).
- Administrators can use the OpenShift installer to deploy a cluster on infrastructure that they prepare and maintain themselves.
- Administrators are responsible for creating and managing their infrastructure

<img src="../imgs/Pre-existing-Infrastructure.svg" />


!!! warning "IPI vs UPI"
    <img src="../imgs/ipi-vs-upi.png" />

## Prerequisites
### General Prerequisites
- Provision a bastion host.
- Generate an **SSH key** on the bastion host.
- Download and install `openshift-install` on the bastion host.
- Download and install `oc` on the bastion host.
- Get the registry `pull-secret`.
- OpenShift cluster nodes must have access to a **Network Time Protocol (NTP)** server.
- Ensure that the infrastructure network firewall fulfills the OpenShift network access requirements.

#### Firewall Prerequisites
Some Ports+Protocol prerequisites for:

- Connectivity Requirements from All Cluster Nodes to All Cluster Nodes
- Connectivity Requirements from All Cluster Nodes to Control Plane Nodes
- API Load Balancer Ports
- Application Ingress Load Balancer Ports

### IPI specific Prerequisites
- Verify infrastructure permissions and quotas.
- For **cloud** environments, a cloud account with required permissions and quotas is required.

### UPI specific Prerequisites
- Configure network services: DNS etc.
- Provide hardware (physical or virtual) for cluster nodes.
- Install RHCOS on cluster nodes.

#### DNS service
When installing OpenShift using the **UPI**, one of the network services that administrators must configure is the **DNS service**. Administrators must configure the DNS service using DNS records. A complete DNS record takes the form `<component>.<cluster_name>.<base_domain>`:

- `<cluster_name>` is the name of the cluster (for instance ocp4).
- `<base_domain>` is the cluster base domain configured in the `install-config.yaml` configuration file (for instance `example.com`).
- The reverse DNS records (PTR) are also required

The following DNS Records are needed:

|Component | Record  | Description|
|:-|:-|:-|
|Kubernetes API |  `api.<cluster_name>.<base_domain>` |  DNS A/AAAA or CNAME and PTR records to identify the API load balancer for the control plane nodes (resolvable by both clients external to the cluster and from all the nodes within the cluster).|
|Kubernetes API |  `api-int.<cluster_name>.<base_domain>` |  DNS A/AAAA or CNAME and PTR records to identify the API load balancer for the control plane nodes (resolvable from all the nodes within the cluster).|
|Routes |  `*.apps.<cluster_name>.<base_domain>` |  Wildcard DNS A/AAAA or CNAME record to identify the Application Ingress load balancer that targets the cluster nodes that run the ingress router pod (resolvable by both clients external to the cluster and from all the nodes within the cluster).|
|Cluster nodes|  `<name>.<cluster_name>.<base_domain>` |  DNS A/AAAA or CNAME and PTR records to identify each cluster node, including **Bootstrap Node** (resolvable by the nodes within the cluster).|


!!! note "BIND"
    **BIND(Berkeley Internet Name Domain)** is one of the most widely used **Domain Name System (DNS)** server software. The BIND configuration is typically managed via two main files:

    - `named.conf`: The primary configuration file defining server settings, zones, and ACLs.
    - Zone files: Contain the actual DNS records for each managed domain. File name and extension are defined in the `*.conf` file

    ```bash
    [root@utility ~]# cat /etc/named.conf

    ...output omitted...
    zone "example.com" {
        type master;
        file "example.com.db";
        allow-update { none; };
    };
    ```
    The zone file `example.com.db` might looks like this:

    ```bash
    [root@utility ~]# cat /var/named/example.com.db

    $TTL  1D
    @     IN  SOA dns.ocp4.example.com. root.example.com. (
                    2019022400 ; serial
                    3h         ; refresh
                    15         ; retry
                    1w         ; expire
                    3h         ; minimum
                )
                IN NS dns.ocp4.example.com.
    dns.ocp4      IN A 192.168.50.254
    api.ocp4      IN A 192.168.50.254
    api-int.ocp4  IN A 192.168.50.254
    *.apps.ocp4   IN A 192.168.50.254
    bootstrap.ocp4 IN A 192.168.50.9
    master01.ocp4 IN A 192.168.50.10
    master02.ocp4 IN A 192.168.50.11
    master03.ocp4 IN A 192.168.50.12
    worker01.ocp4 IN A 192.168.50.13
    worker02.ocp4 IN A 192.168.50.14
    ```
!!! note "Setup and enable BIND"
    1. install Bind
    2. Create config file and zone file
    3. update file permission
    4. restart Bind
    5. Test the config:
        ```bash
        <!-- Check the configuration syntax: -->
        sudo named-checkconf

        <!-- Check zone files -->
        sudo named-checkzone example.com /var/named/example.com.zone

        <!-- Query DNS Server -->
        dig @localhost example.com
        dig @localhost -x 192.168.0.1
        ```


!!! warning
    By default, when installing OpenShift using the full-stack automation method on supported cloud providers, the OpenShift installer automatically performs the DNS service configuration.

    Administrators must configure the DNS service when installing OpenShift using the full-stack automation method on the following infrastructures:
    
    - Red Hat OpenStack Platform cloud provider using an external DNS
    - VMware vSphere
    - Bare metal


# Installation Modes

Depending on the external connectivity of the cluster, administrators can use one of the following installation modes:

## 1. Connected

The cluster nodes have internet access to pull container images from the `quay.io` and `registry.redhat.io` registries. The connected installation mode is supported when using the **full-stack automation (IPI)** or the **pre-existing infrastructure installation (UPI)** methods.

## 2. Disconnected

Administrators use this installation mode when a connected installation is not possible. The OpenShift installer uses a local container registry to pull container images. <br/>➡️ Not all cloud providers support installation in disconnected mode.


1. mirror the required images to the local container registry and obtain the imageContentSources data for your OpenShift version
2. start **OpenShift Installation Process**, you can customize `install-config.yaml` file

# 2. Cluster Installation
无论是 UPI 还是 IPI, 都有一个步骤是Installer完成的，那就是 **Cluster Installation**. 该步骤的具体原理见[这里](./installation_process.md)。

需要的执行步骤有：

1. Fulfill the installation prerequisites.
2. Create the installation directory.<br/>This directory stores all the files created by the OpenShift installer.
```bash
[user@demo ~]$ mkdir ${HOME}/ocp4-cluster
```
3. Create config YAML file. <br/>Use the **Openshift Installer** to create the configuration file (`install-config.yaml`), and save it in the folder `/ocp4-cluster`:
```bash
# generate a cluster SSH public key `ocp4-cluster.pub`
# ⚠️ The **cluster SSH key** enables access from the bastion host to the cluster nodes!
ssh-keygen -t rsa -b 4096 -N '' -f ${HOME}/.ssh/ocp4-cluster
```
Use the SSH key in the `install-config.yaml` file generation
```bash
[user@demo ~]$ openshift-install create install-config \
> --dir=${HOME}/ocp4-cluster
? SSH Public Key /home/user/.ssh/ocp4-cluster.pub
? Platform aws
INFO Credentials loaded from the "default" profile in file "/home/user/.aws/credentials"
? Region us-east-2
? Base Domain mydomain.com
? Cluster Name ocp4
? Pull Secret [? for help] +++++
INFO Install-Config created in: /home/user/ocp4-cluster
```
4. Generate the Kubernetes manifests.<br/>
A Kubernetes manifest is a file that describes one or more Kubernetes API objects, such as Pods, Services, Deployments or MachineConfigs. This cmd takes the `install-config.yaml` as input, and gerates manifest under the folder `/home/user/ocp4-cluster/manifests` and `/home/user/ocp4-cluster/openshift`:
    ```bash
    openshift-install create manifests \
        > --dir=${HOME}/ocp4-cluster
    ```

    !!! warning "Manifest customization"
        You can ONLY modify the manifest if its documented in Redhat support instruction. No free styles pls

5. Generate the ignition configuration files. <br/>This cmd generates ignition files (`master.ign`, `worker.ign`, `bootstrap.ign`) under the folder `/home/user/ocp4-cluster`:
    ```bash
    openshift-install create ignition-configs \
        > --dir=${HOME}/ocp4-cluster
    ```
    
    !!! warning "Ignition files valid time period"
        Ignition files are valid for 24 hours, after which the included certificates expire. If so, create new ignition files.

    !!! warning "ignition file customization"
        You can ONLY modify the ignition file if its documented in Redhat support instruction. No free styles pls 

6. (Only in IPI) Deploy the OpenShift cluster. 
    ```bash
    openshift-install create cluster \
    > --dir=${HOME}/ocp4-cluster --log-level=debug
    ```

    !!! warning "Skip step 6 in UPI Environment"
        In a **UPI / pre-existing infrastructure installation**, you don't nned to run the `openshift-install create cluster` command, because you have already:

        - Installed RHCOS on the cluster nodes (masters, workers, and bootstrap).
        - Set up networking, storage, and load balancers.
        - Configured DNS, ignition files, and more.

        Therefore, just do the following to observe the installation:

        ```bash
        openshift-install wait-for bootstrap-complete \
            > --dir=${HOME}/ocp4-cluster --log-level=debug

        openshift-install wait-for install-complete \
            > --dir=${HOME}/ocp4-cluster --log-level=debug
        ```

7. Verify the OpenShift cluster health. You can start using the `oc` command once the **Bootstrap** has the Kubernetes API running on the **temporary control plane**.

    ```bash
    # 1. Configuring the KUBECONFIG environment variable to use the kubeconfig file.
    export KUBECONFIG=${HOME}/ocp4-cluster/auth/kubeconfig

    # 2. Using the kubeadmin credentials stored in the kubeadmin-password file.

    # 3. Login to the Kubernetes API as cluster-admin
    oc login -u cluster-admin -p [PASSWORD]

    # 4. monitoring:
    # the installation process
    watch 'oc get clusterversion; oc get clusteroperators; \
        > oc get pods --all-namespaces | grep -v -E "Running|Completed"; oc get nodes'

    # how the cluster version operator (CVO) installs the cluster operators
    watch 'oc get clusterversion; oc get clusteroperators; \
        > oc get pods --all-namespaces | grep -v -E "Running|Completed"; oc get nodes'

    # print events
    oc get events -A -w
    ```

<!-- DOUBLE CHECK THIS??? -->
!!! warning 
    **Kubernetes manifests** and **ignition configuration files** are created automatically in the create cluster step, if they do not exist already. (True or False)


# 3. Troubleshooting
!!! info "Stages"
    The cluster installation has 3 main stages:

    1. Bootstrap(Bootkube) Stage:
        - The OpenShift installation process deploys the **bootstrap node**. 
        - The `release-image` **systemd service** running on the **Bootstrap Node** downloads the container images required to start the **temporary control plane**.
        - `bootkube` **systemd service** running on the **Bootstrap Node** starts the **temporary control plane**.
    2. Bootstrap(Temporary Control Plane) Stage
        - The Kubernetes API and the **temporary control plane** are running on the **bootstrap node**. 
        - the **Bootstrap Node** waits until the **control plane nodes** boot and form an **etcd cluster**. 
        - the **Bootstrap Node** schedules the **production control plane** to the control plane nodes.
    3. Production Control Plane Stage: <br/>
        After the **production control plane** runs on the control plane nodes, the **cluster version operator (CVO)** finishes the OpenShift cluster deployment.

## 1. Bootstrap(Bootkube) Stage
At this stage, the Kubernetes API is not yet available, use `ssh` to get logs

Assumes that `192.168.50.9` is the **Bootstrap Node** IP address. Use the cluster SSH private key to connect to the **Bootstrap Node** from the **bastion** host:

```bash
# login to Bootstrap Node
ssh -i ${HOME}/.ssh/ocp4-cluster core@192.168.50.9

# debug two services: release-image & bootkube
journalctl -b -f -u release-image.service -u bootkube.service

# use `crictl` command on the **Bootstrap Node** to look for failed containers and print their logs:
sudo bash
crictl ps -a
crictl ps logs <container_id>
```

## 2. Bootstrap(Temporary Control Plane) Stage
Here, the Kubernetes API is available on the **Bootstrap Node** --> You can use `SSH` to get logs from the **Bootstrap Node** with the following config:

```bash
# export login config
export KUBECONFIG=${HOME}/ocp4-cluster/auth/kubeconfig
```

### download log
```bash
# pull log from bootstrap machine to a *.tar.gz file
openshift-install gather bootstrap \
    > --dir=${HOME}/ocp4-cluster

# unzip and check the log
cd ocp4-cluster 
tar -xvzf log-bundle-20210107135825.tar.gz
cd log-bundle-xxxx
# check the log of the bootstrap systemd services
ls bootstrap/journals/
# check the bootstrap containers
ls bootstrap/containers/
# check the control plane nodes,
ls control-plane/
```

!!! note "UPI for `openshift-install gather`"
    when using UPS, you must specify the IP addresses of the bootstrap and control plane nodes when running the `openshift-install gather` bootstrap command.

    ```bash
    openshift-install gather bootstrap \
        > --dir=${HOME}/ocp4-cluster \
        > --bootstrap <bootstrap_address> \
        > --master <master_1_address> \
        > --master <master_2_address> \
        > --master <master_3_address>" 
    ```

### check log in cluster
```bash
oc get nodes
oc get clusterversion
oc get clusteroperators

oc get events -A -w
oc adm node-logs -u crio master01
oc adm node-logs -u kubelet master01

oc debug node/master01
# after starting the debug node, you can do:
sh-4.2# chroot /host
sh-4.2# journalctl -f
sh-4.2# sudo bash
sh-4.2# crictl ps -a
sh-4.2# crictl logs <container_id>
```

!!! note "crio & kubelet"
    The OpenShift cluster nodes run very few local services because most of the system services run as containers. The main exceptions are the `cri-o` container engine and the `kubelet`, which are **systemd service units**.


# 4. Verifying Installation

[Checklist](./imgs/verifying_oc_installation.pdf)



# 6. Hosted Openshift
OpenShift can be installed using either **IPI** or **UPI** methods, and both options can be used on either **on-premise data centers** or **IaaS cloud providers**.

## IaaS cloud providers that can host Openshift
|Hosted offering | Cloud hosted | Billed by | Managed by | Supported by| Doc|
|:-|:-|:-|:-|:-|:-|
|Microsoft Azure Red Hat OpenShift | Azure | Microsoft | Red Hat and Microsoft | Red Hat and Microsoft|[Doc](https://www.openshift.com/products/azure-openshift)|
|Red Hat OpenShift Dedicated | AWS or GCP | Red Hat (OpenShift), AWS or GCP (Infrastructure) | Red Hat | Red Hat|[Doc](https://www.openshift.com/products/dedicated/)|
|Red Hat OpenShift on IBM Cloud | IBM Cloud | IBM | IBM | Red Hat and IBM|[Doc](https://www.openshift.com/products/openshift-ibm-cloud/)|
|Red Hat OpenShift Service on AWS | AWS | AWS | Red Hat and AWS | Red Hat and AWS|[Doc](https://www.openshift.com/products/amazon-openshift/)|



# oc adm

[reference](https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#cli-administrator-commands)
